{"cells":[{"cell_type":"markdown","metadata":{"id":"drp_xy8Pipyu"},"source":["#### CSC 180 Intelligent Systems\n","\n","#### Dr. Haiquan Chen, Dept of Computer Scicence\n","\n","#### California State University, Sacramento\n"]},{"cell_type":"markdown","metadata":{"id":"3JhHIQqNe4Qs"},"source":["<a name='section0'></a>\n","# Lab 6:  Keras Tuner:  Automatic Hyperparameter Tuning for Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"4o40As8hryIH"},"source":[" For this lab, we will use **Google Colab GPU to speed up training**.  "]},{"cell_type":"markdown","metadata":{"id":"jyTtW4rCHHSi"},"source":[" ***Keras Tuner*** is one of the most popular library developed for tuning the hyperparameters of TensorFlow/Keras neural networks.\n","\n","The Keras Tuner allows to define the search space for the hyperparameters over which the model will be fit, and it returns an optimal set of hyperparameters.\n","\n","Keras Tuner is not part of the `Keras` package and it needs to be installed and imported."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5154,"status":"ok","timestamp":1723482220084,"user":{"displayName":"Victor Chen","userId":"00864070257040861597"},"user_tz":420},"id":"0rnI5QO4MN0k","outputId":"2a3a30b8-05a2-473d-e66a-47f8a6b88518"},"outputs":[],"source":["!pip install -q -U keras-tuner"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5XryNaEBZCAu"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-16 10:23:43.786847: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import tensorflow as tf\n","import keras_tuner as kt"]},{"cell_type":"markdown","metadata":{"id":"bDOJpYwzZGZP"},"source":["### Load MNIST Dataset\n","\n","To demonstrate the use of the Keras Tuner we will work with the MNIST dataset.\n","\n","## MNIST Fashion Data Set\n","\n","The [MNIST Fashion Data Set](https://www.tensorflow.org/datasets/catalog/fashion_mnist) is very popular in the neural network research community.  A sample of it can be seen here:\n","\n","![MNIST Data Set](https://www.researchgate.net/publication/373046669/figure/fig1/AS:11431281180809965@1691723436709/The-FashionMNIST-dataset-consists-of-10-classes-of-monochrome-clothing-items-and-is.png)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1207,"status":"ok","timestamp":1723483230961,"user":{"displayName":"Victor Chen","userId":"00864070257040861597"},"user_tz":420},"id":"sLU9M0eRZCC_","outputId":"c32864e1-0b95-4fd2-ff7c-b6766ae8a725"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","29515/29515 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26421880/26421880 [==============================] - 4s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","5148/5148 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4422102/4422102 [==============================] - 1s 0us/step\n"]}],"source":["(img_train, label_train), (img_test, label_test) = tf.keras.datasets.fashion_mnist.load_data()\n","\n","# grey scale image = 1 (as the last parameter)\n","# color = 3 (last parameter)\n","\n","img_train = img_train.reshape(img_train.shape[0], 28, 28, 1)\n","img_test = img_test.reshape(img_test.shape[0], 28, 28, 1)\n","\n","\n","# Normalize pixel values between 0 and 1\n","img_train = img_train.astype('float32') / 255.0\n","img_test = img_test.astype('float32') / 255.0"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"elapsed":512,"status":"ok","timestamp":1723483563081,"user":{"displayName":"Victor Chen","userId":"00864070257040861597"},"user_tz":420},"id":"-u-aHT35nzgC","outputId":"9659134d-eb88-471d-ea45-c9e2974c4ae8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Image (#0): Which is digit '9'\n"]}],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","sample = 0    #  change this number to select another sample\n","img = img_train[sample]\n","#print(type(digit))\n","#print(digit.shape)\n","\n","\n","plt.imshow(img, cmap='gray')\n","print(\"Image (#{}): Which is digit '{}'\".format(sample, label_train[sample]))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1723483590089,"user":{"displayName":"Victor Chen","userId":"00864070257040861597"},"user_tz":420},"id":"U97lKE9WoIW6","outputId":"f367da08-bec3-402d-cb6a-06b3728e1dc9"},"outputs":[{"data":{"text/plain":["array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["label_train"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"QZg8DqzJipyy"},"outputs":[],"source":["num_classes = 10\n","\n","# Converts a class vector (integers) using One-hot encoding!  Use with categorical_crossentropy.\n","label_train = tf.keras.utils.to_categorical(label_train, num_classes)\n","label_test = tf.keras.utils.to_categorical(label_test, num_classes)"]},{"cell_type":"markdown","metadata":{"id":"8yIFg-EBJQ9b"},"source":["### Model Builder\n","\n","In the cell below, a function called `model_builder` is created, which performs search over two hyperparameters:\n","\n","- Number of neurons in the first Dense layer,\n","- Learning rate.\n","\n","The line `hp_units = hp.Int('units', min_value=32, max_value=512, step=32)` defines a grid search for the number of neurons in the Dense layer in the range [32, 64, 96, ..., 512].\n","\n","Next, a grid search for the learning rate is defined in the range `[1e-2, 1e-3, 1e-4]`."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"w-A-8qDWZCFg"},"outputs":[],"source":["def model_builder(hp):\n","  model = tf.keras.Sequential()\n","\n","  model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1),\n","                 activation='relu',\n","                 input_shape=(28, 28, 1)))\n","\n","  model.add(tf.keras.layers.Flatten())\n","\n","  # Tune the number of units in the first Dense layer\n","  # Choose an optimal value between 32-512\n","  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n","  model.add(tf.keras.layers.Dense(units=hp_units, activation='relu'))\n","  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n","\n","  # Tune the learning rate for the optimizer\n","  # Choose an optimal value from 0.01, 0.001, or 0.0001\n","  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","\n","  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n","                loss=tf.keras.losses.categorical_crossentropy,\n","                metrics=['accuracy'])\n","\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"E49wGjJDLdtS"},"source":["### Hyperparameter Tuning\n","\n","The Keras Tuner has four tuning algorithms available:\n","\n","- RandomSearch Tuner, similar to the Random Grid in scikit-learn performs a random search over a distribution of values for the hyperparameters.\n","- Hyperband Tuner, trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round, to converge to a high-performing model.\n","- BayesianOptimization Tuner, performs BayesianOptimization by creating a probabilistic mapping of the model to the loss function, and iteratively evaluating promising sets of hyperparameters.\n","- Sklearn Tuner, designed for use with scikit-learn models.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":452,"status":"ok","timestamp":1723483728290,"user":{"displayName":"Victor Chen","userId":"00864070257040861597"},"user_tz":420},"id":"BErSgwT8ZCHH","outputId":"a4ec7dd9-543e-403e-c751-5ca4acf4a223"},"outputs":[],"source":["#tuner = kt.Hyperband(model_builder,\n","                 #    objective='val_accuracy',\n","                 #    max_epochs=10,\n","                 #    factor=3)\n","\n","#max_trials represents the number of hyperparameter combinations that will be tested by the tuner,\n","#while execution_per_trial is the number of models that should be built and fit for each trial for robustness purposes.\n","\n","tuner = kt.BayesianOptimization(\n","    model_builder,\n","    objective=\"val_accuracy\",\n","    max_trials=10,\n","    executions_per_trial=1,\n","    directory=\"mnist_kt_test\",\n","    overwrite=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"h9IeeMYBipyz"},"source":["For the sake of class time, let's only do tuning on the first 100 training/test records.  ***Please modify and run the following code on the ENTIRE dataset yourself, which may take about 1 min for each trial for the ENTIRE dataset.***\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96192,"status":"ok","timestamp":1723483852753,"user":{"displayName":"Victor Chen","userId":"00864070257040861597"},"user_tz":420},"id":"ZFuW74JEZCJu","outputId":"02c66557-e782-4659-f6f1-a890eaf69e57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 10 Complete [00h 00m 02s]\n","val_accuracy: 0.5099999904632568\n","\n","Best val_accuracy So Far: 0.5400000214576721\n","Total elapsed time: 00h 00m 44s\n"]}],"source":["tuner.search(img_train[0:100], label_train[0:100], epochs=2, validation_data=(img_test[0:100], label_test[0:100]), callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1723484218091,"user":{"displayName":"Victor Chen","userId":"00864070257040861597"},"user_tz":420},"id":"ido1kjnnMrgt","outputId":"0b074cf6-ff7e-47d3-cd68-14598233ffbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal number of neuron in the Dense layer: 512\n","Optimal learning rate: 0.001\n"]}],"source":["# Get the optimal hyperparameters\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]          # num_trials: Optional number of HyperParameters objects to return.\n","\n","print(f\"Optimal number of neuron in the Dense layer: {best_hps.get('units')}\")\n","print (f\"Optimal learning rate: {best_hps.get('learning_rate')}\")"]},{"cell_type":"markdown","metadata":{"id":"rXnc_9lBp-kR"},"source":["Check this link for API details:  https://keras.io/api/keras_tuner/tuners/base_tuner/"]},{"cell_type":"markdown","metadata":{"id":"Zr7kuBHzN22Y"},"source":["### Train and Evaluate the Model\n","\n","Next, we will use the optimal hyperparameters from the Keras Tuner to create a model, and afterward we will evaluate the accuracy on the test dataset.\n","\n","***Please modify and run the following code on the ENTIRE dataset yourself***\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8783,"status":"ok","timestamp":1723484277329,"user":{"displayName":"Victor Chen","userId":"00864070257040861597"},"user_tz":420},"id":"IMzOKPBSZgv1","outputId":"3b97a92c-81a6-49ca-8014-104eb7172dc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","4/4 - 5s - 1s/step - accuracy: 0.1800 - loss: 2.5098 - val_accuracy: 0.4100 - val_loss: 2.6888\n","Epoch 2/20\n","4/4 - 2s - 603ms/step - accuracy: 0.5800 - loss: 1.8325 - val_accuracy: 0.6200 - val_loss: 1.4105\n","Epoch 3/20\n","4/4 - 0s - 33ms/step - accuracy: 0.7900 - loss: 0.6394 - val_accuracy: 0.6200 - val_loss: 1.2003\n","Epoch 4/20\n","4/4 - 0s - 22ms/step - accuracy: 0.8500 - loss: 0.5382 - val_accuracy: 0.6900 - val_loss: 0.9935\n","Epoch 5/20\n","4/4 - 0s - 36ms/step - accuracy: 0.9200 - loss: 0.2672 - val_accuracy: 0.7000 - val_loss: 1.0001\n","Epoch 6/20\n","4/4 - 0s - 20ms/step - accuracy: 0.9700 - loss: 0.1813 - val_accuracy: 0.6700 - val_loss: 0.9832\n","Epoch 7/20\n","4/4 - 0s - 36ms/step - accuracy: 0.9900 - loss: 0.1203 - val_accuracy: 0.6300 - val_loss: 1.0281\n","Epoch 8/20\n","4/4 - 0s - 34ms/step - accuracy: 0.9900 - loss: 0.0777 - val_accuracy: 0.6800 - val_loss: 1.1102\n","Epoch 9/20\n","4/4 - 0s - 31ms/step - accuracy: 0.9900 - loss: 0.0481 - val_accuracy: 0.6600 - val_loss: 1.1652\n","Epoch 10/20\n","4/4 - 0s - 35ms/step - accuracy: 1.0000 - loss: 0.0288 - val_accuracy: 0.6900 - val_loss: 1.1599\n","Epoch 11/20\n","4/4 - 0s - 21ms/step - accuracy: 1.0000 - loss: 0.0168 - val_accuracy: 0.6800 - val_loss: 1.1892\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7d2d43077460>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Build the model with the optimal hyperparameters and train it on the data for 2 epochs\n","model = tuner.hypermodel.build(best_hps)\n","model.fit(img_train[0:100], label_train[0:100], epochs=20, validation_data=(img_test[0:100], label_test[0:100]), verbose=2, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":212,"status":"ok","timestamp":1723484386324,"user":{"displayName":"Victor Chen","userId":"00864070257040861597"},"user_tz":420},"id":"X-7cpcnG6S3f","outputId":"c96551cf-57d3-44a3-d845-6307196e9db0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6793 - loss: 1.1665 \n","[val loss, val accuracy]: [1.1892025470733643, 0.6800000071525574]\n"]}],"source":["eval_result = model.evaluate(img_test[0:100], label_test[0:100])\n","print(\"[val loss, val accuracy]:\", eval_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pf7mhb34ipy0"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuClass":"premium","machine_shape":"hm","provenance":[{"file_id":"https://github.com/avakanski/Fall-2022-Python-Programming-for-Data-Science/blob/main/Lectures/Theme%203%20-%20Model%20Engineering%20Pipelines/Lecture%2021%20-%20Model%20Selection%2C%20Hyperparameter%20Tuning/Lecture%2021%20-%20Model%20Selection%2C%20Hyperparameter%20Tuning.ipynb","timestamp":1681492116891}]},"gpuClass":"premium","kernelspec":{"display_name":"myenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5 (main, Sep 11 2023, 08:40:08) [Clang 14.0.6 ]"},"vscode":{"interpreter":{"hash":"e47b1a34c05c1e3b83a62d7885c9d1b5ef8a0522d3be0182d0a008ec409b2b3d"}}},"nbformat":4,"nbformat_minor":0}
