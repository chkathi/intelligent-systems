{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D,MaxPooling2D, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import csv\n",
    "\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "from IPython.display import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    \n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "    \n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "    \n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/input/diabetic-data/diabetic_data.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = data.columns[data.isnull().any()]\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical columns with large unique values and drop null value columns\n",
    "df = df.drop(columns=['encounter_id', 'diag_1', 'diag_2', 'diag_3', 'patient_nbr', 'admission_source_id', 'max_glu_serum', 'A1Cresult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the dataset by classes\n",
    "majority_class = df[df['readmitted'] == 'NO']\n",
    "minority_class_30 = df[df['readmitted'] == '<30']\n",
    "minority_class_30_plus = df[df['readmitted'] == '>30']\n",
    "\n",
    "minority_30_upsampled = resample(minority_class_30, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=len(majority_class),  \n",
    "                                 random_state=42)\n",
    "\n",
    "minority_30_plus_upsampled = resample(minority_class_30_plus, \n",
    "                                      replace=True, \n",
    "                                      n_samples=len(majority_class),\n",
    "                                      random_state=42)\n",
    "\n",
    "# Combine the upsampled minority classes with the majority class\n",
    "df_upsampled = pd.concat([majority_class, minority_30_upsampled, minority_30_plus_upsampled])\n",
    "\n",
    "# Shuffle the DataFrame to mix the classes\n",
    "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check the class distribution after oversampling\n",
    "print(df_upsampled['readmitted'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_data = df_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and One Hot_Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_cols = ['num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses', 'time_in_hospital']\n",
    "\n",
    "for col in df_upsampled.columns: \n",
    "    if col in zscore_cols: \n",
    "        encode_numeric_zscore(df_upsampled, col)\n",
    "\n",
    "    elif col == 'readmitted':\n",
    "        label_encoder = LabelEncoder()\n",
    "        df_upsampled['readmitted'] = label_encoder.fit_transform(df_upsampled['readmitted'])\n",
    "\n",
    "    else: \n",
    "         df_upsampled = pd.get_dummies(df_upsampled, columns=[col], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = to_xy(df_upsampled, 'readmitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Features Shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test Features Shape:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ModelCheckpoint outside the loop\n",
    "# filepath = '/content/drive/MyDrive/CS/intelligent-systems/p2/best_weights.keras'\n",
    "filepath = './best_weights.keras'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=filepath, verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "# otherwise new model will override after each loop\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999,epsilon=1e-07\n",
    "\n",
    "# Build network\n",
    "model = Sequential()\n",
    "model.add(Dense(4096, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# add early stopping within the loop\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=2,epochs=100, batch_size=100)\n",
    "\n",
    "print('Training finished...Loading the best model')\n",
    "print()\n",
    "model.load_weights('./best_weights.keras') # load weights from best model\n",
    "\n",
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "\n",
    "y_true = np.argmax(y_test,axis=1)\n",
    "\n",
    "score = metrics.accuracy_score(y_true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final accuracy: {}\".format(score))\n",
    "print(classification_report(y_true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN input must be 4 dimensions\n",
    "x_train_cnn = x_train.reshape((x_train.shape[0], 1, x_train.shape[1], 1))\n",
    "x_test_cnn = x_test.reshape((x_test.shape[0], 1, x_train.shape[1], 1))\n",
    "\n",
    "# One hot encoded output\n",
    "y_train_cnn = y_train\n",
    "y_test_cnn = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"x_train_cnn shape: {x_train_cnn.shape}\")\n",
    "print(f\"x_test_cnn shape: {x_test_cnn.shape}\")\n",
    "print(f\"y_train_cnn shape: {y_train_cnn.shape}\")\n",
    "print(f\"y_test_cnn shape: {y_test_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999,epsilon=1e-07)\n",
    "\n",
    "# Initialize the model\n",
    "cnn = Sequential()\n",
    "\n",
    "# Layer 1: First Convolutional Layer\n",
    "cnn.add(Conv2D(filters=32, kernel_size=(1, 5), strides=(1, 1), activation='relu', \n",
    "               input_shape=(1, x_train_cnn.shape[2], 1)))  # Input shape matches your data\n",
    "cnn.add(MaxPooling2D(pool_size=(1, 2)))  # Pooling reduces feature width by half\n",
    "\n",
    "# Layer 2: Second Convolutional Layer\n",
    "cnn.add(Conv2D(filters=64, kernel_size=(1, 7), strides=(1, 1), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(1, 2)))  # Further reduce width by half\n",
    "\n",
    "# Layer 3: Third Convolutional Layer\n",
    "cnn.add(Conv2D(filters=128, kernel_size=(1, 15), strides=(1, 1), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(1, 2)))  # Reduce further for higher-level features\n",
    "\n",
    "# Flatten: Convert 2D features into 1D for dense layers\n",
    "cnn.add(Flatten())\n",
    "\n",
    "# Dense Layers: Fully connected layers\n",
    "cnn.add(Dense(1024, activation='relu'))  \n",
    "cnn.add(Dense(512, activation='relu'))  \n",
    "cnn.add(Dense(256, activation='relu')) \n",
    "cnn.add(Dense(128, activation='relu')) \n",
    "cnn.add(Dense(3, activation='softmax')) \n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "cnn.fit(x_train_cnn,y_train_cnn,validation_data=(x_test_cnn,y_test_cnn),callbacks=[monitor,checkpointer],verbose=2,epochs=100, batch_size=64)\n",
    "\n",
    "print('CNN - Training finished...Loading the best model')\n",
    "print()\n",
    "# cnn.load_weights('./cnn_best_weights1.keras') # load weights from best model\n",
    "\n",
    "# Measure accuracy\n",
    "pred_cnn = cnn.predict(x_test_cnn)\n",
    "pred_cnn = np.argmax(pred_cnn,axis=1)\n",
    "\n",
    "y_true_cnn = np.argmax(y_test_cnn,axis=1)\n",
    "\n",
    "score_cnn = metrics.accuracy_score(y_true_cnn, pred_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final accuracy: {}\".format(score_cnn))\n",
    "print(classification_report(y_true_cnn, pred_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './cnn_best_weights2.keras'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=filepath, verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999,epsilon=1e-07)\n",
    "\n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv2D(filters=32, kernel_size=(1, 5), strides=(1, 1), activation='relu', \n",
    "               input_shape=(1, x_train_cnn.shape[2], 1))) \n",
    "cnn.add(MaxPooling2D(pool_size=(1, 2)))  \n",
    "\n",
    "cnn.add(Conv2D(filters=64, kernel_size=(1, 7), strides=(1, 1), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(1, 2)))  \n",
    "\n",
    "cnn.add(Conv2D(filters=128, kernel_size=(1, 15), strides=(1, 1), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(1, 2)))  \n",
    "\n",
    "cnn.add(Conv2D(filters=256, kernel_size=(1, 15), strides=(1, 1), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(1, 2))) \n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(4096, activation='relu'))  \n",
    "cnn.add(Dropout(.2))\n",
    "cnn.add(Dense(1024, activation='relu'))  \n",
    "cnn.add(Dense(512, activation='relu')) \n",
    "cnn.add(Dense(128, activation='relu')) \n",
    "cnn.add(Dense(3, activation='softmax')) \n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "cnn.fit(x_train_cnn,y_train_cnn,validation_data=(x_test_cnn,y_test_cnn),callbacks=[monitor,checkpointer],verbose=2,epochs=100, batch_size=256)\n",
    "\n",
    "print('CNN - Training finished...Loading the best model')\n",
    "print()\n",
    "cnn.load_weights('./cnn_best_weights2.keras') # load weights from best model\n",
    "\n",
    "# Measure accuracy\n",
    "pred_cnn2 = cnn.predict(x_test_cnn)\n",
    "pred_cnn2 = np.argmax(pred_cnn2,axis=1)\n",
    "\n",
    "y_true_cnn2 = np.argmax(y_test_cnn,axis=1)\n",
    "\n",
    "score_cnn2 = metrics.accuracy_score(y_true_cnn2, pred_cnn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final accuracy: {}\".format(score_cnn2))\n",
    "print(classification_report(y_true_cnn2, pred_cnn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = transformer_data\n",
    "t_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(t_data, random_state=42, test_size=0.2)\n",
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)\n",
    "train_data_file = \"/kaggle/working/train_data.csv\"\n",
    "test_data_file = \"/kaggle/working/test_data.csv\"\n",
    "CSV_HEADER = t_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_cols = ['num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses', 'time_in_hospital']\n",
    "\n",
    "t_data['weight'] = 0\n",
    "\n",
    "for col in zscore_cols: \n",
    "    t_data['weight']  = t_data['weight'] + t_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data['weight'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the weighted column.\n",
    "WEIGHT_COLUMN_NAME = \"weight\"\n",
    "\n",
    "zscore_cols = ['num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses', 'time_in_hospital']\n",
    "\n",
    "# A list of numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = zscore_cols\n",
    "\n",
    "# Dynamically determine categorical features by excluding numeric features.\n",
    "CATEGORICAL_FEATURE_NAMES = [\n",
    "    col for col in transformer_data.columns \n",
    "    if col not in NUMERIC_FEATURE_NAMES and not col.startswith('readmitted') \n",
    "    and not col.startswith('weight')\n",
    "]\n",
    "\n",
    "# Generate vocabulary for categorical features.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    feature: sorted(list(transformer_data[feature].astype(str).unique())) \n",
    "    for feature in CATEGORICAL_FEATURE_NAMES\n",
    "}\n",
    "\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "\n",
    "TARGET_FEATURE_NAME = \"readmitted\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\"<30\", \">30\", \"NO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CATEGORICAL_FEATURE_NAMES), len(NUMERIC_FEATURE_NAMES), len(FEATURE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=1\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target_index, weights\n",
    "\n",
    "\n",
    "lookup_dict = {}\n",
    "for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "    # Create a lookup to convert a string values to an integer indices.\n",
    "    # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "    lookup = layers.StringLookup(\n",
    "        vocabulary=vocabulary, mask_token=None, num_oov_indices=1\n",
    "    )\n",
    "    lookup_dict[feature_name] = lookup\n",
    "\n",
    "\n",
    "def encode_categorical(batch_x, batch_y, weights):\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n",
    "\n",
    "    return batch_x, batch_y, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = (\n",
    "        tf_data.experimental.make_csv_dataset(\n",
    "            csv_file_path,\n",
    "            batch_size=batch_size,\n",
    "            column_names=CSV_HEADER,\n",
    "            column_defaults=COLUMN_DEFAULTS,\n",
    "            label_name=TARGET_FEATURE_NAME,\n",
    "            num_epochs=1,\n",
    "            header=False,\n",
    "            na_value=\"?\",\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        .map(prepare_example, num_parallel_calls=tf_data.AUTOTUNE, deterministic=False)\n",
    "        .map(encode_categorical)\n",
    "    )\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(batch_x, batch_y, weights):\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n",
    "\n",
    "    return batch_x, batch_y, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = (\n",
    "        tf_data.experimental.make_csv_dataset(\n",
    "            csv_file_path,\n",
    "            batch_size=batch_size,\n",
    "            column_names=CSV_HEADER,\n",
    "            column_defaults=COLUMN_DEFAULTS,\n",
    "            label_name=TARGET_FEATURE_NAME,\n",
    "            num_epochs=1,\n",
    "            header=False,\n",
    "            na_value=\"?\",\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        .map(prepare_example, num_parallel_calls=tf_data.AUTOTUNE, deterministic=False)\n",
    "        .map(encode_categorical)\n",
    "    )\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "    \n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"int32\"\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, embedding_dims):\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert a string values to an integer indices.\n",
    "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(inputs[feature_name])\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = ops.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list\n",
    "\n",
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer())\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = ops.stack(encoded_categorical_feature_list, axis=1)\n",
    "    # Concatenate numerical features.\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = ops.arange(start=0, stop=num_columns, step=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "       # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=partial(\n",
    "                layers.LayerNormalization, epsilon=1e-6\n",
    "            ),  # using partial to provide keyword arguments before initialization\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "    # Apply layer normalization to the numerical features.\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    # Prepare the input for the final MLP block.\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "\n",
    "     # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization,\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(3, activation=\"softmax\", name=\"softmax\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "# keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
